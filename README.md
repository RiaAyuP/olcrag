# Offline Chat with File using Ollama, Langchain, and Flask

Currently not offline as I am trying the example of loading website. Currently doesn't have memory.

## Comparison
[Chat with File using Streamlit and Ollama](https://github.com/RiaAyuP/ollamarag) 

## References
1. [Sam Witteveen's Tutorial](https://github.com/samwit/langchain-tutorials/tree/main/2024/gemma2_local_rag)
2. [Build a Local RAG](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)