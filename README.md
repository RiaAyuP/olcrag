# Offline Chat with File using Ollama, Langchain, and Flask

- Surprisingly I learned how to do HTML and CSS this time.
- Currently the chatbot is not offline as I am trying the example of loading website. 
- Currently doesn't have memory.

## Next Step
1. [Upload Files](https://flask.palletsprojects.com/en/2.3.x/patterns/fileuploads/)
2. [Chat History](https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/)

## Comparison
[Chat with File using Streamlit and Ollama](https://github.com/RiaAyuP/ollamarag) 

## References
1. [Sam Witteveen's Tutorial](https://github.com/samwit/langchain-tutorials/tree/main/2024/gemma2_local_rag)
2. [LangChain's Build a Local RAG](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)
3. [LangChain's How to Q&A with RAG](https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag)